:PROPERTIES:
:ID:       f3e6cddf-4d91-416b-a4e7-a12d24a05ff8
:mtime:    20220401231202
:ctime:    20220326235451
:END:
#+title: Lagrange multiplier
#+filetags: :stub:

If we want to find the maximum of some function \( f(\mathbf{x}) \) along some constraint \(
g(\mathbf{x}) = 0 \), then the gradient of \( f \) must be proportional to the gradient of \( g \).

Indeed, if this were not the case, we could move along \( g \) to increase the value of \( f \).

This means that at this maximal point, there must exist a \( \lambda \) such that

\[
\mathcal{L}(\mathbf{x}, \lambda) = f(\mathbf{x}) - \lambda g(\mathbf{x})
\]

satisfies \( \nabla \mathcal{L}(\mathbf{x}, \lambda) = 0 \).

In other words, we are now in the situation of a standard maximization problem. The additional
unknown \( \lambda \) may now be solved with the help of the constraint equation \( g(\mathbf{x}) = 0 \).

[[id:cdd6f3d1-16fd-4cd9-935e-0d4d5d98f5b2][Envelope theorem]]
[[id:a2e15f7e-6e79-41e3-bc6a-1cdc1c5ceb91][Interpretation of Lagrange multipliers]]
