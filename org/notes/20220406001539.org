:PROPERTIES:
:ID:       32e79adf-f95b-4d9d-aa61-afbbb611b215
:mtime:    20220409002259
:ctime:    20220406001539
:END:
#+title: @Information geometry
#+filetags: :inbox:

[[id:af2992e5-49f4-486e-aafe-2c7d5b7f9acb][Information geometry]]
https://math.ucr.edu/home/baez/information/information_geometry_1.html

* Part 1
Starting from the [[id:1e8f5ce6-7cf1-4957-bbdb-a01a67cf621d][Gibbs distribution]]:

We can derive the [[id:f16ab982-752c-40c2-bf0d-8f674f00ba58][Covariance matrix]] from the logarithm of the [[id:a1085e61-0103-411a-8ac4-f5319da6b3c8][Partition function]] by

\[
\frac{\partial^2 \log Z}{\partial \lambda_i \partial \lambda_j} = \langle X_i X_j \rangle - \langle
X_i \rangle \langle X_j \rangle
\]

if

\[
Z = \mathrm{tr} \left( e^{-\lambda_1 X_1 - ... - \lambda_n X_n} \right)
\]

Unfortunately, in [[id:9a56000d-da8c-4cba-82cc-6038fe352323][Quantum mechanics]], the operators do not commute.

Therefore, we need to define

\[
g_{ij} = \mathrm{Re} \langle (X_i - \langle X_i \rangle)(X_j - \langle X_j \rangle) \rangle
\]

to end up with an object satisfying \( g_{ij} = g_{ji} \).

# TODO: Understand this statementj
If \( x_i = \langle X_i \rangle \), the points \( x = (x_1, ..., x_n) \in \mathbb{R}^n \) form a
[[id:61fa204e-4b7e-4adf-b7f0-0e9b94f7f8a9][Manifold]].

The metric \( g_{ij} \) is also positive and when it is positive-definite, it is an [[id:2faceb77-f478-4f5b-9a88-f109b32b9763][Inner product]]
on the [[id:7d44f76c-6425-45d8-a0ec-f3427dc29fdc][Tangent space]] at the point \( x \), hence defining a [[id:474eb746-d6ff-4e70-a8fd-eadc2eeca52d][Riemannian metric]] on \( \mathbb{R}^n \).

Now it turns out that this is the [[id:83601649-3af6-42c0-b54b-032150ce4787][Fisher information metric]].

* Part 2
Consider the [[id:1e8f5ce6-7cf1-4957-bbdb-a01a67cf621d][Gibbs distribution]] (the unique entropy maximizing distribution given constraints on
the average)

\[
P = \frac{\exp \left( -\sum\limits_{i=1}^n \lambda_i X_i \right)}{Z}
\]

Now we do not actually need to take the derivatives of the partition function, but we can take

\[
\frac{\partial \log P}{\partial \lambda_i} = -X_i + \langle X_i \rangle
\]

So now we can rewrite

\begin{equation*}
\begin{align}
g_{ij}
&= \mathrm{Re} \langle (X_i - \langle X_i \rangle)(X_j - \langle X_j \rangle) \rangle \\
&= \mathrm{Re} \left\langle \frac{\partial \log P}{\partial \lambda_i} \frac{\partial \log P}{\partial \lambda_j} \right\rangle
\end{align}
\end{equation*}

This is precisely the [[id:83601649-3af6-42c0-b54b-032150ce4787][Fisher information metric]].

* Part 3
Consider the converse where we start with \( P \), some function from a [[id:61fa204e-4b7e-4adf-b7f0-0e9b94f7f8a9][Manifold]] to the space of
[[id:623d4a27-b14a-487d-98e7-4245de36e4c6][Probability measures]] (these are [[id:768c533d-dd46-4853-a5ea-13031fcbdb2d][Statistical manifolds]]).

Now write

\[
P = \frac{e^{-A}}{Z}
\]

Then

\[
\frac{1}{Z} \frac{\partial Z}{\partial \lambda_i} = -\frac{1}{Z} \mathrm{tr} \left( e^{-A}
\frac{\partial A}{\partial \lambda_i} \right) = - \left\langle \frac{\partial A}{\partial \lambda_i} \right\rangle
\]

If we now set

\[
X_i = \frac{\partial A}{\partial \lambda_i}
\]

we recover once again the definition

\[
\frac{\partial \log P}{\partial \lambda_i} = -X_i + \langle X_i \rangle
\]

There is actually a [[id:156ed55e-61e3-4eab-9dfc-314fbc36978a][Gauge transformation]] we can do: \( A \mapsto A + f \) for \( f: M \mapsto
\mathbb{R} \) a smooth function doesn't change the physics of what we are doing, although it does
change \( Z \). This leaves the metric invariant.

Because of the ability to do a [[id:6e45d13d-b622-47e4-a531-f4d68b68a8d1][Wick rotation]], this [[id:156ed55e-61e3-4eab-9dfc-314fbc36978a][Gauge transformation]] is closely related to the
[[id:156ed55e-61e3-4eab-9dfc-314fbc36978a][Gauge transformation]] to phases in [[id:9a56000d-da8c-4cba-82cc-6038fe352323][Quantum mechanics]].

* Part 4
* Part 5
Working out the example of a [[id:a7071b80-7c24-407a-a488-5a0dcf048935][Harmonic oscillator]].

In this case \( H = \frac{1}{2} (p^2 + q^2) \) and the [[id:1e8f5ce6-7cf1-4957-bbdb-a01a67cf621d][Gibbs distribution]] becomes

\[
P = \frac{\exp \left( - \frac{1}{2} (p^2 + q^2) \right)}{Z}
\]

So

\begin{equation*}
\begin{align}
\frac{\partial \log P}{\partial p} &= -p \\
\frac{\partial \log P}{\partial q} &= -q
\end{align}
\end{equation*}

Now clearly the cross-terms vanish because the [[id:d59f2d74-dbe3-46e3-8a5e-9849350d24bc][Normal distribution]] is symmetric.

Hence we are left with

\[
g_{pp} = \frac{1}{Z} \int\limits_{-\infty}^{\infty} p^2 e^{-\frac{1}{2} p^2} \, dp = 1
\]

Similary for \( g_{qq} \), so we end up with the standard metric

\( g = dp^2 + dq^2 \)

If we have some Gaussian with covariance, we probably get some rotated metric? Should be really
easy to check.

* Part 6
Information gain is given by

\[
S(p, q) = \int\limits_{\Omega} p(\omega) \log \frac{p(\omega)}{q(\omega)} \, d\omega
\]

This is also known as the [[id:827f101b-80d4-43da-a3a5-33fd4b944379][Kullback-Leibler divergence]].

It tells you how much information you have learned by updating from distribution \( q \rightarrow p
\). This is not necessarily symmetric. For example, learning that a fair coin always comes up heads
leads you to learn one bit of information. The converse however, leads to an information gain of
infinite bits, because you assumed a priori that coming up tails was completely impossible.

In terms of the [[id:7ca91bb4-6cd8-4e6b-8c9d-323f1604b3e8][Radon-Nikodym derivative]], we have

\[
S(\mu, \nu) = \int\limits_{\Omega} \log \frac{d\mu}{d\nu} \, d\mu
\]

This is not exactly a metric:
- \( S(\mu, \nu) \geq 0 \)
- \( S(\mu, \nu) = 0 \) if and only if \( \mu = \nu \)

but \( S(\mu, \nu) \neq S(\nu, \mu) \). On top of that, it also fails the [[id:b258c443-ae40-4ab4-a79e-771d404c5234][Triangle inequality]].

* Part 7
A [[id:768c533d-dd46-4853-a5ea-13031fcbdb2d][Statistical manifold]] is a manifold whose points are hypotheses about some situation. For example,
for a [[id:a52537ca-6373-4385-a4fd-0a0777e95dd7][Bernoulli distribution]], we could take \( [0, 1] \) as our [[id:768c533d-dd46-4853-a5ea-13031fcbdb2d][Statistical manifold]]. The manifold
parametrizes these hypotheses. This is fundamental to the subject of [[id:81acbfcf-5f2f-4f8e-9a23-49e332108570][Parametric statistics]].

Consider the [[id:827f101b-80d4-43da-a3a5-33fd4b944379][Kullback-Leibler divergence]] \( S(p, q) \).

Let \( p = q + h \)

Then

\[
p \log \frac{p}{q} = (q + h) \log \left( 1 + \frac{h}{q} \right) = h + \frac{h^2}{q}
\]

Since both

\[
\int\limits_{\Omega} p \, d\omega = \int\limits_{\Omega} q \, d\omega = 1
\]

it must be true that

\[
\int\limits_{\Omega} h \, d\omega = 0
\]

\[
\int\limits_{\Omega} p \log \frac{p}{q} \, d\omega = \int\limits_{\Omega} \left( h + \frac{h^2}{q}
\right) \, d\omega = \int\limits_{\Omega} \frac{h^2}{q} \, d\omega
\]

Hence \( S(q + h, q) \approx \int\limits_{\Omega} \frac{h^2}{q} \, d\omega \) and the first-order
variation vanishes.

# TODO: Finish the calculation with a fresh mind. Don't even need the polarization trick?

I would need to calculate something like

\[
\lim_{h_1, h_2 \rightarrow 0} \frac{f(x + h_1) - 2f(x) + f(x - h_2)}{h_1 h_2}
\]

* Part 8
If we assume we have a number of species of animals that increase in size based on an exponential
equation

\[
\frac{dP_i}{dt} = f_i P_i
\]

with \( f_i \) the fitness of the i-th genotype, we may rewrite this to the [[id:ba1bac70-ca8a-4f54-a98e-c2deef2f4e20][Replicator equation]]
using

\[
p_i = \frac{P_i}{\sum\limits_i P_i}
\]

This gives

\[
\frac{dp_i}{dt} = \left( f_i - \langle f \rangle \right) p_i
\]

where \( \langle f \rangle = \sum\limits_i f_i p_i \) is the mean of the [[id:90b731c5-40f0-4ee1-913a-c4f13597dfc0][Fitness function]]. The functions \( f_i \)
may be functions of all other \( P_j \) as well.

# TODO: How does this connect exactly to the KL divergence?

To connect to [[id:39253c51-52f6-46bf-bbc1-29650f407acb][Evolutionary game theory]], we can take

\[
f_i = \sum\limits_{i, j} A_{ij} p_j
\]

where \( A_{ij} \) is the [[id:27c2a906-1f2c-4d6d-b3ea-8feb4bfeb4e2][Payoff matrix]].

# TODO: How exactly does this relate to network theory?
This also relates to [[id:7313940d-ef08-4dac-85e6-273427ad7e92][Network theory]] somehow.
