#+title:      Renormalization
#+date:       [2022-03-22 Tue 17:09]
#+filetags:   
#+identifier: 20220322T170906

* [[denote:20220322T105321][Renormalization]]
** Economics
- Macroeconomics. Trying to find out theories that govern the global state, even though there might
  be many micro interactions that we do not know about. Only using the big picture variables like
  Gini, GDP, etc.
- These higher level variables might be better at explaining which policies to change, what is
  actually happening in the big picture
- Micro to macro is a good example to show how coarse-graining works. Many different micro
  descriptions can still give the same macro description. In the same "equivalence class".
- Entropy goes down. Macro is much more certain about the world than micro

** Coarse-graining picture
Simplify an image by averaging over all pixels in a block. Coarse-graining. Many different pictures
ultimately lead to the same set of coarse-grained pictures.

If we coarse-grain, the image takes up less space. For example, [[denote:20220322T175512][JPEG compression]] is really good for compression
(with loss) without losing too much information. JPEG takes the "long-wavelength" of the picture or
the very quickly oscillating differences, high-frequency components.

JPEG respects the way the human eye records information.

In this case: We think about the problem we want to solve and invent an algorithm.

** Markov chains
Basic form for a time series. Now we take a low-time resolution of the [[denote:20220210T195925][Markov chain]] as our
coarse-graining. The Markov chain is the model that describes this time series and we will
"renormalize" it.

Markov chains are memoryless. They are completely determined by the current state. We can
coarse-grain by simply taking the Markov transition matrix \( P \) and taking \( P^2 \) or \( P^3
\), etc. This is the idea of decimation coarse-graining.

Models tends to flow to fixed points. Corresponding model changes less and less. Markov chains have
an especially nice limit (if there are no absorbing states or clusters of chains). All of the
incoming arrows have the same probability, so we have a strong compression of the information we
need to describe the model.

If you wait long enough, the Markov chain will move towards the stationary distribution. This
distribution does not depend on the initial state.

Draw a simple picture of a two-by-two Markov chain?

Some conditions: There has to be exactly one eigenvalue of absolute length one. For example Markov
chain which moves from A -> B -> A all the time has two eigenvectors with eigenvalues of absolute
value 1.

Draw the state space and draw the renormalization flow in (pA->A, pB->B) space. Boundaries do not
flow to the attractive fixed points.

Sometimes it is impossible that your Markov chain comes from a 2x2 finer-grained Markov chain (can
check that there is no matrix \( Q \) such that \( Q^n = P \). However, it is possible to
coarse-grine from a model space with more memory, more states.

The moral of the story is that sometimes the finer-grained model is not just a different model, but
a completely different model class. You need to give that model better sophistication than you had
in the coarse-grained version.

For Markov chains coarse-graining simplified the story, but we have examples where it is the
opposite. We have to enlarge the model space: Simplifying data makes model more complex.

* [[denote:20220323T214500][Cellular automata]]
[[denote:20220323T214432][Elementary cellular automaton]] are simple, but can still give rise to complex behaviour. For
example, It turns out that [[denote:20220323T220229][Rule 110 is Turing complete]].

Whereas Markov chains have a memoryless property, cellular automata are dependent on arbitrarily
large inputs in the past. This means correlations between far neighbours can exist over long enough
periods of time.

In fact, if we drop out an intermediate timestep as we would do when we coarse-grain, we move out
of the model space of elementary cellular automata, because our cell now also depends on the value
of the next-nearest neighbours.

Left at Israeli and Goldenfeld I.

* [[denote:20220329T210550][Ising model]]
Coarse-graining sometimes leads to a form of non-locality.

By taking a very natural coarse-graining, what we find is that the exact solution that we want, is
no longer in the same model class, in fact by throwing out some of the information, we have allowed
arbitrary long distances to communicate with each other.

When I throw out information in my local environment, what I might do accidentally, is bring in
information from much much further away. By simplifying the data, I have not only made the model
complex, but even possibly pathological.

The Ising model was invented to describe the behaviour of atoms on a lattice. We do a 2D lattice.
Assume each atom can be in spin up or spin down.

Probability of particular configuration:

\[
\mathbb{P}[\{\sigma_m\}] = \frac{1}{Z(\beta)} e^{\beta \sum\limits_{i, j=1}^N J_{ij} \sigma_i \sigma_j}
\]

where \( J \) is like the [[denote:20220329T211310][Adjacency matrix]]. Although the Ising model started as a model to describe
behaviour of atoms in a lattice, it has been used in various fields as a simple model to describe
local interactions.

- Approximate renormalization: renormalization might not commute with evolution anymore.
- But in the Ising model case, the approximate solution works pretty well

The fixed points have a scale invariant property. We get fluctuations at all scales.

* [[denote:20220329T215305][Krohn-Rhodes theorem]]
Stack of three cards. Cycle operations and swap operation (\( S_3 \) [[denote:20220218T232930][Group (mathematics)]]).

Coarse-grain into equivalence class ([[denote:20220329T215709][Normal subgroup]]?) by the sign of the [[denote:20220329T215735][Permutation]]. Then cycle
does not and swap changes the sign.

The [[denote:20220329T232621][Composition series]] of the [[denote:20220329T232721][Symmetric group]] \( S_3 \) is

\[
\{ e \} \triangleleft A_3 \triangleleft S_3
\]

where \( A_3 \) is the [[denote:20220329T232735][Alternating group]] of order 3. The [[denote:20220329T232750][Jordan-HÃ¶lder theorem]] implies that this
decomposition is unique up to isomorphism.

The [[denote:20220329T215305][Krohn-Rhodes theorem]] says this still holds if we use a [[denote:20220329T233017][Semi-group]] instead. In other words,
instead of only allowing reversible operations, we can also allow irreversible operations. All you
have to do, is add the "universal reset".

Holonomy decompositions?

The greater number of resets you needed somehow described the essential complexity of the system.

* [[denote:20220329T233621][Quantum electrodynamics]]
We can tell a good part of the story of renormalization in QED and the fact that virtual particles
exist in the vacuum and modify the bare coupling, through thermal fluctuations.

[[denote:20220329T234044][Plasma]]: Separating electrons from protons. Say a gas is separated into its negatively and
positively charged parts.

Assume overall the charge is zero and that each positive part has the same absolute charge, but
opposite sign of the negative part. For example, hydrogen plasma.

If sufficiently mixed: In any small part, we should find as many positive as negative parts. Assume
[[denote:20220329T234429][Thermal equilibrium]] (what is the difference between thermal equilibrium and thermodynamic equilibrium?)

Then

\[
n_s(x) = n_0 e^{-\beta E(x)}
\]

where \( E(x) = q_s \Phi \).

So

\begin{equation*}
\begin{align}
n_i &= n_0 e^{-\beta q \Phi} \\
n_e &= n_0 e^{\beta q \Phi}
\end{align}
\end{equation*}

Now what happens if we introduce a test particle? This is in analogy to the real particle moving
around in the cloud of virtual particles.

How does this thermal equilibrium interact with the introduced test particle?

If we have a point charge, then

\[
\Phi = \frac{Q}{4 \pi \epsilon_0 r}
\]

Now \( \delta \rho = \delta \rho_{external} + q (\delta n_i - \delta n_e) \).

Using the equations for \( n_s \), we find

\[
\delta \rho = \delta \rho_{external} - 2 \beta q^2 n_0 \delta \Phi
\]

So now due to [[denote:20220330T231920][Maxwell's equations]]

\[
\nabla^2 \delta \Phi = -\frac{1}{\epsilon_0} \left[ \delta \rho_{external} - 2 \beta q^2 n_0 \delta
\Phi \right]
\]

This is solved by

\[
\delta \Phi = \frac{Q}{4 \pi \epsilon_0 r} e^{-\sqrt{2} r / \lambda_D }
\]

where \( \lambda_D = 1 / \sqrt{\beta q^2 n_0} \) is known as the [[denote:20220330T232839][Debye length]]. This looks very much
like the [[denote:20220330T233028][Yukawa potential]]?

So the further and further you go, the less you will see the particle. And the particle becomes
invisible when the distance is much larger than the [[denote:20220330T232839][Debye length]].

Alternatively, you can think of \( Q \) being rescaled as \( Q \rightarrow Q e^{-\sqrt{2} r /
\lambda_D} \).

The problem with [[denote:20220329T233621][QED]] is that the equations actually blow up due to UV divergences.

In some cases, the approximate scheme like for the Ising model does not work and our theories keep
generating new and new terms that we cannot put into our old theory. These theories are [[denote:20220330T233801][Non-renormalizable theory]].

* Keeping the things that matter
Coarse-graining data: Connecting models at different scales is what renormalization is. Often we
know what we want renormalization to do.

[[denote:20220331T000650][Utility function]]: [[denote:20220330T234703][Rate-distortion theory]]. Efficient representation vs. good representation. [[denote:20220331T000152][Mutual
information]]. You have to be explicit about which information you want to retain, that is the key.


* Interesting exercises:
- How can we apply renormalization to the 1D Ising model? This should be exact instead of an
  approximate renormalization.

* Reference
https://www.youtube.com/watch?v=rXnZ-HFoOz8&list=PLF0b3ThojznTzAA7bfLWh4RKzRrwNF4L0

