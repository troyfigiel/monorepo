:PROPERTIES:
:ID:       8ca59335-a8bc-4f8b-8236-ea6c133d6079
:mtime:    20220225223632
:ctime:    20220218221659
:END:
#+title: Chebyshev's inequality gives sharp bounds for arbitrary distributions

For each \( k \) we can consider the distribution

\[
X =
\begin{cases}
-1, & \text{with probability } \frac{1}{2k^2} \\
0, & \text{with probability } 1 - \frac{1}{k^2} \\
1, & \text{with probability } \frac{1}{2k^2} \\
\end{cases}
\]

Then \( \mu = 0 \) and \( \sigma = \frac{1}{k} \) so

\[
\mathbb{P} \left( | X - \mu | \geq k \sigma \right) = \mathbb{P} \left( | X | \geq 1 \right) = \frac{1}{k^2}
\]

showing that there exist distributions for which Chebyshev's inequality is sharp.

* TODO
It is true that Chebyshev's inequality is sharp iff the distribution is a linear transformation of the above example. Why?
